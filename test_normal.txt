Train on 2501 samples, val on 2510 samples, with batch size 24.
Epoch 1/30
104/104 [==============================] - 354s 3s/step - loss: 1768.8868 - val_loss: 509.6128
Epoch 2/30
104/104 [==============================] - 329s 3s/step - loss: 211.1692 - val_loss: 178.9696
Epoch 3/30
104/104 [==============================] - 327s 3s/step - loss: 98.0778 - val_loss: 93.9529
Epoch 4/30
104/104 [==============================] - 321s 3s/step - loss: 65.2197 - val_loss: 60.4863
Epoch 5/30
104/104 [==============================] - 325s 3s/step - loss: 50.6593 - val_loss: 48.1970
Epoch 6/30
104/104 [==============================] - 323s 3s/step - loss: 42.7045 - val_loss: 41.2385
Epoch 7/30
104/104 [==============================] - 322s 3s/step - loss: 37.3803 - val_loss: 37.5109
Epoch 8/30
104/104 [==============================] - 322s 3s/step - loss: 33.9263 - val_loss: 34.1336
Epoch 9/30
104/104 [==============================] - 321s 3s/step - loss: 31.0832 - val_loss: 31.6179
Epoch 10/30
104/104 [==============================] - 318s 3s/step - loss: 29.4613 - val_loss: 30.5022
Epoch 11/30
104/104 [==============================] - 318s 3s/step - loss: 28.1966 - val_loss: 29.9452
Epoch 12/30
104/104 [==============================] - 317s 3s/step - loss: 26.7500 - val_loss: 29.2501
Epoch 13/30
104/104 [==============================] - 318s 3s/step - loss: 25.7945 - val_loss: 26.8620
Epoch 14/30
104/104 [==============================] - 318s 3s/step - loss: 24.6780 - val_loss: 26.9315
Epoch 15/30
104/104 [==============================] - 317s 3s/step - loss: 24.3621 - val_loss: 25.7423
Epoch 16/30
104/104 [==============================] - 317s 3s/step - loss: 23.3505 - val_loss: 25.0400
Epoch 17/30
104/104 [==============================] - 317s 3s/step - loss: 22.8912 - val_loss: 24.4241
Epoch 18/30
104/104 [==============================] - 316s 3s/step - loss: 22.2171 - val_loss: 24.6555
Epoch 19/30
104/104 [==============================] - 317s 3s/step - loss: 21.5704 - val_loss: 23.2850
Epoch 20/30
104/104 [==============================] - 318s 3s/step - loss: 21.2531 - val_loss: 22.4243
Epoch 21/30
104/104 [==============================] - 317s 3s/step - loss: 21.0050 - val_loss: 23.6310
Epoch 22/30
104/104 [==============================] - 318s 3s/step - loss: 20.3969 - val_loss: 22.5435
Epoch 23/30
104/104 [==============================] - 316s 3s/step - loss: 19.9404 - val_loss: 21.9814
Epoch 24/30
104/104 [==============================] - 318s 3s/step - loss: 19.8510 - val_loss: 22.6927
Epoch 25/30
104/104 [==============================] - 317s 3s/step - loss: 19.8362 - val_loss: 22.1605
Epoch 26/30
104/104 [==============================] - 319s 3s/step - loss: 19.4517 - val_loss: 20.8986
Epoch 27/30
104/104 [==============================] - 318s 3s/step - loss: 19.2619 - val_loss: 21.9961
Epoch 28/30
104/104 [==============================] - 318s 3s/step - loss: 18.6745 - val_loss: 21.6165
Epoch 29/30
104/104 [==============================] - 318s 3s/step - loss: 18.7419 - val_loss: 21.0036
Epoch 30/30
104/104 [==============================] - 316s 3s/step - loss: 18.5088 - val_loss: 21.6207

loaded weights logs/000/test_fake_trained_weights_stage_1.h5
aeroplane: 0.1468
bicycle: 0.1580
bird: 0.1129
boat: 0.0090
bottle: 0.0168
bus: 0.1489
car: 0.4024
cat: 0.1502
chair: 0.0154
cow: 0.0166
diningtable: 0.0154
dog: 0.2832
horse: 0.2378
motorbike: 0.1808
person: 0.3238
pottedplant: 0.0106
sheep: 0.0059
sofa: 0.0987
train: 0.1438
tvmonitor: 0.2188
mAP: 0.1348

Train on 2501 samples, val on 2510 samples, with batch size 20.
Epoch 31/60
125/125 [==============================] - 340s 3s/step - loss: 17.1770 - val_loss: 18.9149
Epoch 32/60
125/125 [==============================] - 323s 3s/step - loss: 16.5976 - val_loss: 18.4793
Epoch 33/60
125/125 [==============================] - 324s 3s/step - loss: 16.0894 - val_loss: 18.5539
Epoch 34/60
125/125 [==============================] - 325s 3s/step - loss: 15.8675 - val_loss: 18.4413
Epoch 35/60
125/125 [==============================] - 325s 3s/step - loss: 15.6818 - val_loss: 18.2008
Epoch 36/60
125/125 [==============================] - 325s 3s/step - loss: 15.7334 - val_loss: 18.2304
Epoch 37/60
125/125 [==============================] - 324s 3s/step - loss: 15.2424 - val_loss: 18.0865
Epoch 38/60
125/125 [==============================] - 324s 3s/step - loss: 15.4637 - val_loss: 17.8522
Epoch 39/60
125/125 [==============================] - 324s 3s/step - loss: 15.2985 - val_loss: 18.3043
Epoch 40/60
125/125 [==============================] - 324s 3s/step - loss: 15.1265 - val_loss: 18.3668
Epoch 41/60
125/125 [==============================] - 324s 3s/step - loss: 15.0791 - val_loss: 18.4155

Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.
Epoch 42/60
125/125 [==============================] - 324s 3s/step - loss: 15.1237 - val_loss: 18.0121
Epoch 43/60
125/125 [==============================] - 325s 3s/step - loss: 14.6987 - val_loss: 17.9366
Epoch 44/60
125/125 [==============================] - 323s 3s/step - loss: 14.7421 - val_loss: 17.8581

Epoch 00044: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.
Epoch 45/60
125/125 [==============================] - 324s 3s/step - loss: 14.7984 - val_loss: 17.7365
Epoch 46/60
125/125 [==============================] - 325s 3s/step - loss: 14.5655 - val_loss: 18.0486
Epoch 47/60
125/125 [==============================] - 323s 3s/step - loss: 14.8773 - val_loss: 17.9434
Epoch 48/60
125/125 [==============================] - 324s 3s/step - loss: 14.6184 - val_loss: 17.9613

Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.
Epoch 49/60
125/125 [==============================] - 325s 3s/step - loss: 14.6060 - val_loss: 17.9540
Epoch 50/60
125/125 [==============================] - 324s 3s/step - loss: 14.7500 - val_loss: 17.6516
Epoch 51/60
125/125 [==============================] - 321s 3s/step - loss: 14.8003 - val_loss: 17.9288
Epoch 52/60
125/125 [==============================] - 323s 3s/step - loss: 14.6815 - val_loss: 17.6329
Epoch 53/60
125/125 [==============================] - 324s 3s/step - loss: 14.6269 - val_loss: 18.1084
Epoch 54/60
125/125 [==============================] - 324s 3s/step - loss: 14.7222 - val_loss: 17.4889
Epoch 55/60
125/125 [==============================] - 324s 3s/step - loss: 14.6986 - val_loss: 18.0455
Epoch 56/60
125/125 [==============================] - 322s 3s/step - loss: 14.5090 - val_loss: 18.2392
Epoch 57/60
125/125 [==============================] - 323s 3s/step - loss: 14.7002 - val_loss: 17.8107

Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.
Epoch 58/60
125/125 [==============================] - 322s 3s/step - loss: 14.7427 - val_loss: 17.7262
Epoch 59/60
125/125 [==============================] - 323s 3s/step - loss: 14.7052 - val_loss: 17.8591
Epoch 60/60
124/125 [============================>.] - ETA: 1s - loss: 14.5310
Epoch 00060: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.

loaded weights logs/000/test_fake_trained_weights_final.h5
aeroplane: 0.3062
bicycle: 0.4162
bird: 0.2475
boat: 0.1266
bottle: 0.1308
bus: 0.3508
car: 0.5309
cat: 0.4603
chair: 0.0786
cow: 0.2252
diningtable: 0.1804
dog: 0.2783
horse: 0.4261
motorbike: 0.4164
person: 0.4064
pottedplant: 0.0799
sheep: 0.1522
sofa: 0.3234
train: 0.4038
tvmonitor: 0.2732
mAP: 0.2907