Train on 2501 samples, val on 2510 samples, with batch size 24.
Epoch 1/30
104/104 [==============================] - 365s 4s/step - loss: 1237.7158 - val_loss: 288.3639
Epoch 2/30
104/104 [==============================] - 345s 3s/step - loss: 147.2572 - val_loss: 126.5531
Epoch 3/30
104/104 [==============================] - 344s 3s/step - loss: 75.7595 - val_loss: 65.8806
Epoch 4/30
104/104 [==============================] - 339s 3s/step - loss: 53.9558 - val_loss: 47.7536
Epoch 5/30
104/104 [==============================] - 344s 3s/step - loss: 43.0264 - val_loss: 39.4620
Epoch 6/30
104/104 [==============================] - 343s 3s/step - loss: 37.2093 - val_loss: 35.5314
Epoch 7/30
104/104 [==============================] - 341s 3s/step - loss: 33.4701 - val_loss: 33.2006
Epoch 8/30
104/104 [==============================] - 344s 3s/step - loss: 30.8237 - val_loss: 31.4521
Epoch 9/30
104/104 [==============================] - 346s 3s/step - loss: 28.9380 - val_loss: 29.9782
Epoch 10/30
104/104 [==============================] - 342s 3s/step - loss: 27.4212 - val_loss: 28.0606
Epoch 11/30
104/104 [==============================] - 345s 3s/step - loss: 26.1506 - val_loss: 26.4589
Epoch 12/30
104/104 [==============================] - 344s 3s/step - loss: 25.2511 - val_loss: 25.6778
Epoch 13/30
104/104 [==============================] - 340s 3s/step - loss: 24.5678 - val_loss: 25.5720
Epoch 14/30
104/104 [==============================] - 336s 3s/step - loss: 23.5770 - val_loss: 25.9269
Epoch 15/30
104/104 [==============================] - 342s 3s/step - loss: 22.7409 - val_loss: 24.0915
Epoch 16/30
104/104 [==============================] - 336s 3s/step - loss: 22.4554 - val_loss: 23.5155
Epoch 17/30
104/104 [==============================] - 336s 3s/step - loss: 21.8018 - val_loss: 23.4951
Epoch 18/30
104/104 [==============================] - 330s 3s/step - loss: 21.4460 - val_loss: 23.0552
Epoch 19/30
104/104 [==============================] - 331s 3s/step - loss: 21.0385 - val_loss: 22.7088
Epoch 20/30
104/104 [==============================] - 332s 3s/step - loss: 20.4278 - val_loss: 22.4976
Epoch 21/30
104/104 [==============================] - 329s 3s/step - loss: 20.3361 - val_loss: 21.6879
Epoch 22/30
104/104 [==============================] - 330s 3s/step - loss: 19.8449 - val_loss: 22.1419
Epoch 23/30
104/104 [==============================] - 332s 3s/step - loss: 19.2886 - val_loss: 22.1615
Epoch 24/30
104/104 [==============================] - 329s 3s/step - loss: 19.5724 - val_loss: 21.0149
Epoch 25/30
104/104 [==============================] - 327s 3s/step - loss: 19.0253 - val_loss: 21.9668
Epoch 26/30
104/104 [==============================] - 330s 3s/step - loss: 18.6268 - val_loss: 20.5565
Epoch 27/30
104/104 [==============================] - 327s 3s/step - loss: 18.6432 - val_loss: 21.6905
Epoch 28/30
104/104 [==============================] - 327s 3s/step - loss: 18.5728 - val_loss: 21.0951
Epoch 29/30
104/104 [==============================] - 330s 3s/step - loss: 18.6298 - val_loss: 20.7853
Epoch 30/30
104/104 [==============================] - 332s 3s/step - loss: 17.8562 - val_loss: 20.1063

loaded weights logs/000/test_fake_trained_weights_stage_1.h5
aeroplane: 0.1577
bicycle: 0.1853
bird: 0.1340
boat: 0.0741
bottle: 0.0363
bus: 0.1851
car: 0.3065
cat: 0.2365
chair: 0.0231
cow: 0.0786
diningtable: 0.0110
dog: 0.1731
horse: 0.2475
motorbike: 0.1148
person: 0.3220
pottedplant: 0.0152
sheep: 0.1229
sofa: 0.1206
train: 0.1797
tvmonitor: 0.1839
mAP: 0.1454

Unfreeze all of the layers.
Train on 2501 samples, val on 2510 samples, with batch size 20.
Epoch 31/60
125/125 [==============================] - 374s 3s/step - loss: 17.1465 - val_loss: 18.7752
Epoch 32/60
125/125 [==============================] - 355s 3s/step - loss: 16.2947 - val_loss: 18.8418
Epoch 33/60
125/125 [==============================] - 359s 3s/step - loss: 15.9086 - val_loss: 18.4722
Epoch 34/60
125/125 [==============================] - 358s 3s/step - loss: 15.8178 - val_loss: 18.4463
Epoch 35/60
125/125 [==============================] - 359s 3s/step - loss: 15.6111 - val_loss: 18.3216
Epoch 36/60
125/125 [==============================] - 359s 3s/step - loss: 15.6235 - val_loss: 18.1208
Epoch 37/60
125/125 [==============================] - 360s 3s/step - loss: 15.3999 - val_loss: 18.3841
Epoch 38/60
125/125 [==============================] - 358s 3s/step - loss: 15.2604 - val_loss: 18.0284
Epoch 39/60
125/125 [==============================] - 366s 3s/step - loss: 15.0355 - val_loss: 18.4293
Epoch 40/60
125/125 [==============================] - 360s 3s/step - loss: 14.7273 - val_loss: 18.0939
Epoch 41/60
125/125 [==============================] - 356s 3s/step - loss: 14.9062 - val_loss: 18.0430

Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.
Epoch 42/60
125/125 [==============================] - 360s 3s/step - loss: 14.9301 - val_loss: 18.0137
Epoch 43/60
125/125 [==============================] - 357s 3s/step - loss: 14.7649 - val_loss: 17.8410
Epoch 44/60
125/125 [==============================] - 359s 3s/step - loss: 14.4905 - val_loss: 17.9787
Epoch 45/60
125/125 [==============================] - 358s 3s/step - loss: 14.4039 - val_loss: 18.1628
Epoch 46/60
125/125 [==============================] - 359s 3s/step - loss: 14.4605 - val_loss: 17.7351
Epoch 47/60
125/125 [==============================] - 360s 3s/step - loss: 14.5070 - val_loss: 17.6709
Epoch 48/60
125/125 [==============================] - 359s 3s/step - loss: 14.5252 - val_loss: 17.6660
Epoch 49/60
125/125 [==============================] - 358s 3s/step - loss: 14.2939 - val_loss: 18.0668
Epoch 50/60
125/125 [==============================] - 358s 3s/step - loss: 14.2246 - val_loss: 17.5619
Epoch 51/60
125/125 [==============================] - 360s 3s/step - loss: 14.3421 - val_loss: 18.2001
Epoch 52/60
125/125 [==============================] - 360s 3s/step - loss: 14.3140 - val_loss: 17.6952
Epoch 53/60
125/125 [==============================] - 360s 3s/step - loss: 14.3686 - val_loss: 18.1647

Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.
Epoch 54/60
125/125 [==============================] - 358s 3s/step - loss: 14.5661 - val_loss: 17.8272
Epoch 55/60
125/125 [==============================] - 358s 3s/step - loss: 14.2368 - val_loss: 17.9783
Epoch 56/60
125/125 [==============================] - 358s 3s/step - loss: 14.3361 - val_loss: 17.5842

Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.
Epoch 57/60
125/125 [==============================] - 363s 3s/step - loss: 14.5417 - val_loss: 18.0296
Epoch 58/60
125/125 [==============================] - 361s 3s/step - loss: 14.4361 - val_loss: 17.8057
Epoch 59/60
125/125 [==============================] - 363s 3s/step - loss: 14.4792 - val_loss: 17.9588

Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.
Epoch 60/60
125/125 [==============================] - 357s 3s/step - loss: 14.1940 - val_loss: 17.5820

logs/000/test_fake_trained_weights_final.h5
aeroplane: 0.3062
bicycle: 0.4425
bird: 0.2289
boat: 0.1231
bottle: 0.1512
bus: 0.3746
car: 0.5349
cat: 0.4673
chair: 0.0904
cow: 0.2522
diningtable: 0.1952
dog: 0.3202
horse: 0.4090
motorbike: 0.4308
person: 0.4245
pottedplant: 0.0673
sheep: 0.1937
sofa: 0.3982
train: 0.4191
tvmonitor: 0.2861
mAP: 0.3058
